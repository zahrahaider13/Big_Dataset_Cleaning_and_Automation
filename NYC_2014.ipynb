{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5231c06",
   "metadata": {},
   "source": [
    "# **NYC Parking Violations 2014: Data Cleaning & Analysis**\n",
    "\n",
    "**Author:** Zahra Haider\n",
    "\n",
    "---\n",
    "\n",
    "# **📌 Introduction**\n",
    "\n",
    "New York City's parking violation data is a treasure trove of insights—but only if we can tame its chaos. The 2014 dataset, with 9.1 million raw records, contained inconsistencies, missing values, and structural issues that made analysis impossible.\n",
    "\n",
    "🔹 **Start:** The data held potential but was buried under formatting inconsistencies and noise.\n",
    "\n",
    "🔹 **Conflict:** Without cleaning, any analysis would be unreliable or misleading.\n",
    "\n",
    "🔹 **Solution:** A rigorous, step-by-step pipeline to transform raw data into an analysis-ready asset.\n",
    "\n",
    "---\n",
    "\n",
    "## **⚙️ Step 1: Set Up the Environment**\n",
    "\n",
    "**Goal:** Prepare the workspace for efficient large-scale processing.\n",
    "\n",
    "**Libraries:** Pandas for data manipulation.\n",
    "\n",
    "**Configuration:** Defined input/output files and a 500K-row chunk size to avoid memory crashes.\n",
    "\n",
    "**Why Chunks?** The dataset’s size (~9M rows) made full-load processing infeasible.\n",
    "\n",
    "**Key Insight:** Chunking balances performance and resource limits.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d244fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Config\n",
    "INPUT_FILE = \"Parking_Violations_Issued_-_Fiscal_Year_2014.csv\"\n",
    "OUTPUT_FILE = \"cleaned_nyc_parking_tickets_2014.csv\"\n",
    "CHUNK_SIZE = 500_000  # Process in chunks to avoid memory crashes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2318fd67",
   "metadata": {},
   "source": [
    "## **🧹 Step 2: Define Cleaning Logic**\n",
    "\n",
    "### **2.1 Columns to Drop (Low Value)**\n",
    "\n",
    "***Problem:*** Low-value columns (e.g., ``feet_from_curb``, ``violation_legal_code``) had >80% nulls or were irrelevant.\n",
    "\n",
    "***Solution:*** Pruned upfront to streamline processing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8059a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_DROP = [\n",
    "    'intersecting_street', 'feet_from_curb', 'violation_legal_code',\n",
    "    'violation_in_front_of_or_opposite', 'from_hours_in_effect', 'to_hours_in_effect'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0ff8f0",
   "metadata": {},
   "source": [
    "### **2.2 Valid U.S. States**\n",
    "\n",
    "***Problem:*** Invalid registration states (e.g., \"XX\", \"ZZ\") polluted the dataset.\n",
    "\n",
    "***Solution:*** Restricted to **50 U.S. states + DC** using a whitelist.\n",
    "\n",
    "***Impact:*** Removed 56,771 invalid records (0.6% of total).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40619e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_STATES = [\n",
    "    'AL','AK','AZ','AR','CA','CO','CT','DE','FL','GA','HI','ID','IL','IN','IA',\n",
    "    'KS','KY','LA','ME','MD','MA','MI','MN','MS','MO','MT','NE','NV','NH','NJ',\n",
    "    'NM','NY','NC','ND','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VT',\n",
    "    'VA','WA','WV','WI','WY','DC'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfdc883",
   "metadata": {},
   "source": [
    "## **🚀 Step 3: The Cleaning Function**\n",
    "\n",
    "### **3.1 Standardize Column Names**\n",
    "\n",
    "***Problem:*** Mixed formats (``ViolationCode`` vs. ``violation_code``).\n",
    "\n",
    "***Solution:*** Lowercase with underscores for consistency.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6cf18765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(chunk):\n",
    "    chunk.columns = chunk.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade0fc3b",
   "metadata": {},
   "source": [
    "### **3.2 Drop useless columns**\n",
    "\n",
    "***Problem:*** Redundant or empty columns wasted memory.\n",
    "\n",
    "***Solution:*** Eliminated 6 columns upfront.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7b725ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(chunk):\n",
    "    return chunk.drop(columns=[col for col in COLUMNS_TO_DROP if col in chunk.columns], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c2113",
   "metadata": {},
   "source": [
    "### **3.3 Handle Missing Data**\n",
    "\n",
    "***Problem:*** Critical fields like ``summons_number`` had nulls.\n",
    "\n",
    "***Solution:*** Dropped rows with null keys (56,771 rows removed).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81118d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_data(chunk):\n",
    "    chunk = chunk.dropna(subset=['summons_number', 'issue_date'])\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23777629",
   "metadata": {},
   "source": [
    "### **3.4. Clean strings (plate_id, state, vehicle make)**\n",
    "\n",
    "***Problem:*** Text fields had inconsistent casing (e.g., ``\"toyota\"`` vs. ``\"TOYOTA\"``).\n",
    "\n",
    "***Solution:*** Uppercased and trimmed whitespace.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "edc7fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_strings(chunk):\n",
    "    str_cols = ['plate_id', 'registration_state', 'vehicle_make', 'vehicle_body_type']\n",
    "    for col in str_cols:\n",
    "        if col in chunk.columns:\n",
    "            chunk[col] = chunk[col].astype(str).str.strip().str.upper()\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd513e5",
   "metadata": {},
   "source": [
    "### **3.5. Filter valid states only**\n",
    "\n",
    "***Problem:*** Non-U.S. plates skewed results.\n",
    "\n",
    "***Solution:*** Applied the state whitelist.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7cd4e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_states(chunk):\n",
    "    if 'registration_state' in chunk.columns:\n",
    "        chunk = chunk[chunk['registration_state'].isin(VALID_STATES)]\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd72bcb",
   "metadata": {},
   "source": [
    "### **3.6. Fix dates (convert to datetime, extract features)**\n",
    "\n",
    "***Problem:*** Dates were strings (e.g., ``\"01/12/2014\"``).\n",
    "\n",
    "***Solution:*** Converted to datetime and extracted year/month/day/weekday.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "644fca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dates(chunk):\n",
    "    if 'issue_date' in chunk.columns:\n",
    "        chunk['issue_date'] = pd.to_datetime(chunk['issue_date'], errors='coerce')\n",
    "        chunk = chunk[chunk['issue_date'].notna()]\n",
    "        chunk['issue_year'] = chunk['issue_date'].dt.year\n",
    "        chunk['issue_month'] = chunk['issue_date'].dt.month\n",
    "        chunk['issue_day'] = chunk['issue_date'].dt.day\n",
    "        chunk['issue_dayofweek'] = chunk['issue_date'].dt.dayofweek\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b48fcd7",
   "metadata": {},
   "source": [
    "### **3.7. Convert numbers (violation_code, street codes)**\n",
    "\n",
    "***Problem:*** Numeric columns (e.g., ``violation_code``) were sometimes strings.\n",
    "\n",
    "***Solution:*** Coerced to numeric type.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc3f269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_numerics(chunk):\n",
    "    num_cols = ['violation_code', 'street_code1', 'street_code2', 'street_code3']\n",
    "    for col in num_cols:\n",
    "        if col in chunk.columns:\n",
    "            chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n",
    "    return chunk      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8257725b",
   "metadata": {},
   "source": [
    "### **3.8: MAIN CLEANING FUNCTION**\n",
    "\n",
    "***Orchestration:*** Sequentially applied all steps to each chunk.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5344a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_chunk(chunk):\n",
    "    chunk = clean_column_names(chunk)\n",
    "    chunk = drop_columns(chunk)\n",
    "    chunk = handle_missing_data(chunk)\n",
    "    chunk = clean_strings(chunk)\n",
    "    chunk = filter_states(chunk)\n",
    "    chunk = fix_dates(chunk)\n",
    "    chunk = fix_numerics(chunk)\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec91824",
   "metadata": {},
   "source": [
    "## **📂 Step 4: Process in Chunks & Save**\n",
    "\n",
    "***Problem:*** The dataset couldn’t fit in memory.\n",
    "\n",
    "***Solution:*** Processed 500K-row chunks, tracking duplicates globally.\n",
    "\n",
    "---\n",
    "\n",
    "***Output:*** Saved incrementally to ``cleaned_nyc_parking_tickets_2014.csv``.\n",
    "\n",
    "***Duplicate Handling:*** Used a ``seen_summons`` set to avoid cross-chunk duplicates.\n",
    "\n",
    "***Result:*** 9,043,506 cleaned rows (99.4% retention).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_summons = set()  # Track duplicates across chunks\n",
    "\n",
    "with open(OUTPUT_FILE, 'w') as f:  # Clear output file\n",
    "    pass\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(INPUT_FILE, chunksize=CHUNK_SIZE)):\n",
    "    print(f\"Processing chunk {i+1}...\")\n",
    "    \n",
    "    cleaned_chunk = clean_chunk(chunk)\n",
    "    \n",
    "    # Remove duplicates across chunks\n",
    "    if 'summons_number' in cleaned_chunk.columns:\n",
    "        cleaned_chunk = cleaned_chunk[~cleaned_chunk['summons_number'].isin(seen_summons)]\n",
    "        seen_summons.update(cleaned_chunk['summons_number'].tolist())\n",
    "    \n",
    "    # Save to CSV (header only for first chunk)\n",
    "    cleaned_chunk.to_csv(\n",
    "        OUTPUT_FILE,\n",
    "        mode='a',\n",
    "        index=False,\n",
    "        header=(i == 0)\n",
    "    )\n",
    "\n",
    "print(f\"✅ Done! Cleaned data saved to: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac725f",
   "metadata": {},
   "source": [
    "## **🔎 Step 5: Post-Cleaning Checks**\n",
    "\n",
    "### **5.1 Verify Row Count**\n",
    "\n",
    "***Original:*** 9,100,277 rows → Cleaned: 9,043,506 rows.\n",
    "\n",
    "***Loss:*** 56,771 rows (invalid states/key nulls).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ffd7aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_count_rows(filename):\n",
    "    count = 0\n",
    "    for chunk in pd.read_csv(filename, chunksize=10_000, dtype='unicode'):\n",
    "        count += len(chunk)\n",
    "    return count\n",
    "\n",
    "original_rows = safe_count_rows(INPUT_FILE) - 1\n",
    "cleaned_rows = safe_count_rows(OUTPUT_FILE) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "20b0458d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset rows: 9,100,277\n",
      "Cleaned dataset rows: 9,043,506\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(f\"Original dataset rows: {original_rows:,}\")\n",
    "print(f\"Cleaned dataset rows: {cleaned_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca93bf",
   "metadata": {},
   "source": [
    "### **5.2 Handle Remaining Nulls**\n",
    "\n",
    "***Problem:*** Some columns still had nulls (e.g., ``violation_location:`` 27%).\n",
    "\n",
    "***Solution:***\n",
    "\n",
    "1. ***Dropped:*** Columns with >90% nulls (e.g., ``latitude``).\n",
    "\n",
    "2. ***Imputed:*** Placeholders for moderate nulls (e.g., ``UNKNOWN_LOCATION``).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fafa3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_nulls(filename, sample_size=100000):\n",
    "    sample = pd.read_csv(filename, nrows=sample_size)\n",
    "    return sample.isnull().mean() * 100  # Returns percentages\n",
    "\n",
    "null_percentages = estimate_nulls(OUTPUT_FILE)\n",
    "print(\"Estimated null percentages:\")\n",
    "print(null_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db654dbc",
   "metadata": {},
   "source": [
    "### **5.3 Final Validation**\n",
    "\n",
    "***Null Check:*** Confirmed 0 remaining nulls in critical fields.\n",
    "\n",
    "***Sample Inspection:*** Manually verified random rows.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "adc84697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Nulls handled! Updated file saved to cleaned_nyc_parking_tickets_2014.csv\n"
     ]
    }
   ],
   "source": [
    "# ---- NULL HANDLING STEP (add this right before saving your final file) ---- #\n",
    "\n",
    "# 1. Drop columns with >90% nulls (adjust list as needed)\n",
    "df = df.drop(columns=[\n",
    "    'time_first_observed', \n",
    "    'violation_post_code',\n",
    "    'no_standing_or_stopping_violation',\n",
    "    'latitude',\n",
    "    'longitude'\n",
    "], errors='ignore')  # 'errors=ignore' skips missing columns\n",
    "\n",
    "# 2. Fill remaining nulls\n",
    "df = df.fillna({\n",
    "    'violation_location': 'UNKNOWN_LOCATION',\n",
    "    'issuer_command': 'UNKNOWN_COMMAND',\n",
    "    'house_number': 'N/A',\n",
    "    'violation_county': 'UNKNOWN_COUNTY'\n",
    "})\n",
    "\n",
    "# Continue with your existing saving logic\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"✅ Nulls handled! Updated file saved to {OUTPUT_FILE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cacd97d",
   "metadata": {},
   "source": [
    "**Verification (Add This After Saving)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8950e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check for remaining nulls\n",
    "print(\"\\nRemaining null counts:\")\n",
    "print(pd.read_csv(OUTPUT_FILE, nrows=1).columns)  # Check columns first\n",
    "null_check = pd.read_csv(OUTPUT_FILE).isnull().sum()\n",
    "print(null_check[null_check > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fe2e53",
   "metadata": {},
   "source": [
    "**📂 Your Clean Data File**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2bb71f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"cleaned_nyc_parking_tickets_2014.csv\"  # Ready-to-use!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dfe4e0",
   "metadata": {},
   "source": [
    "## **🔍 Step 6: Quick Check Before Analysis**\n",
    "\n",
    "**Final Output:**\n",
    "\n",
    "1. ***File:*** cleaned_nyc_parking_tickets_2014.csv.\n",
    "\n",
    "2. ***Size:*** ~1.2 GB (compressed: ~300 MB).\n",
    "\n",
    "3. ***Columns:*** 28 (down from 34)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(OUTPUT_FILE)\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(\"\\nSample data:\")\n",
    "print(df.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8198989",
   "metadata": {},
   "source": [
    "## **➡️ Next Steps: Analysis Opportunities**\n",
    "\n",
    "With clean data, we can now explore:\n",
    "\n",
    "**1. Temporal Trends**\n",
    "\n",
    "- ***Question:*** Are tickets higher on weekends or holidays?\n",
    "\n",
    "- ***Method:*** Group by issue_dayofweek or issue_month.\n",
    "\n",
    "**2. Vehicle Patterns**\n",
    "\n",
    "- ***Question:*** Do certain car makes/colors get ticketed more?\n",
    "\n",
    "- ***Method:*** Aggregate by vehicle_make or vehicle_color.\n",
    "\n",
    "**3. Geospatial Hotspots**\n",
    "\n",
    "- ***Question:*** Which precincts issue the most tickets?\n",
    "\n",
    "- ***Method:*** Map violation_precinct counts (if coordinates were kept).\n",
    "\n",
    "**4. Enforcement Analysis**\n",
    "\n",
    "- ***Question:*** Are certain officers (issuer_code) more active?\n",
    "\n",
    "- ***Method:*** Rank top issuers by violation count.\n",
    "\n",
    "---\n",
    "\n",
    "## **🎯 Conclusion**\n",
    "\n",
    "This project transformed a messy 9M-record dataset into a reliable foundation for analysis. By addressing nulls, inconsistencies, and scalability, we’ve unlocked actionable insights into NYC’s parking violations.\n",
    "\n",
    "**Key Takeaway:** Clean data isn’t just about removing noise—it’s about revealing truth.\n",
    "\n",
    "**Tools Used:** Python, Pandas, Systematic Chunking.\n",
    "\n",
    "**Author:** Zahra Haider\n",
    "\n",
    "---\n",
    "\n",
    "***🚀 Ready for analysis!***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e0d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
