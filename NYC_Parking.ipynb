{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b8d689",
   "metadata": {},
   "source": [
    "# **NYC Parking Violations Data Cleaning Pipeline**\n",
    "\n",
    "**Author: Zahra Haider**\n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Introduction**\n",
    "\n",
    "Parking violations in New York City are a goldmine of insights‚Äîif the data is cleaned properly. Raw datasets from 2014 to 2017 contained inconsistencies, missing values, and structural changes year-over-year. This project systematically cleans and standardizes the data, ensuring it‚Äôs ready for analysis.\n",
    "\n",
    "üîπ Start: The raw data was messy‚Äîmissing values, inconsistent formats, and duplicate entries.\n",
    "\n",
    "üîπ Conflict: Without cleaning, analysis would be unreliable or impossible.\n",
    "\n",
    "üîπ Solution: A structured, step-by-step pipeline that handles each issue systematically.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Setup & Config**\n",
    "\n",
    "Before diving into cleaning, we set up the environment:\n",
    "\n",
    "***Folder Structure:*** Created ``raw_data`` (input) and ``cleaned_data`` (output) directories.\n",
    "\n",
    "***Year Configuration:*** Defined years ``(2014-2017)`` and chunk size ``(100,000 rows)`` for memory efficiency.\n",
    "\n",
    "***Valid States:*** Restricted registration states to ``NY, NJ, CT, PA, FL, CA`` (2014 only).\n",
    "\n",
    "***Why?*** Raw data was too large to load at once, and state restrictions were year-specific.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960940ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Folder setup\n",
    "os.makedirs(\"raw_data\", exist_ok=True)\n",
    "os.makedirs(\"cleaned_data\", exist_ok=True)\n",
    "\n",
    "# Year configuration\n",
    "YEARS = [2014, 2015, 2016, 2017]\n",
    "CHUNK_SIZE = 100_000  # Process 100K rows at a time\n",
    "VALID_STATES = ['NY','NJ','CT','PA','FL','CA']  # From 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb4857b",
   "metadata": {},
   "source": [
    "## **2. Step-by-Step Cleaning Functions**\n",
    "\n",
    "### **üìå STEP 1: Column Names**\n",
    "\n",
    "***Problem:*** Column names had inconsistent formatting (e.g., ``Violation Code`` vs. ``violation_code``).\n",
    "\n",
    "***Solution:*** Standardized to lowercase with underscores ``(clean_column_names()``).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57fc1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 1: COLUMN NAMES =====\n",
    "def clean_column_names(chunk):\n",
    "    \"\"\"Standardize column names (lowercase with underscores)\"\"\"\n",
    "    chunk.columns = [col.strip().lower().replace(\" \", \"_\") for col in chunk.columns]\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e046ef1",
   "metadata": {},
   "source": [
    "### **üìå STEP 2: Column Removal**\n",
    "\n",
    "***Problem:*** Some columns had >80% nulls or were irrelevant (e.g., ``feet_from_curb``).\n",
    "\n",
    "***Solution:*** Dropped useless columns (``drop_useless_columns()``).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 2: COLUMN REMOVAL =====\n",
    "def drop_useless_columns(chunk):\n",
    "    \"\"\"Drop low-value columns (2014 criteria)\"\"\"\n",
    "    cols_to_drop = [\n",
    "        'intersecting_street',  # 2014: Inconsistent data\n",
    "        'feet_from_curb',       # 2014: >80% nulls\n",
    "        'violation_legal_code', # 2014: Not used in analysis\n",
    "        'time_first_observed'   # 2014: 95% nulls\n",
    "    ]\n",
    "    return chunk.drop(columns=[c for c in cols_to_drop if c in chunk.columns], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ebc32",
   "metadata": {},
   "source": [
    "### **üìå STEP 3: Null Handling**\n",
    "\n",
    "***Problem:*** Critical fields like ``violation_location`` had ~27% nulls.\n",
    "\n",
    "***Solution:*** Filled nulls with placeholders (e.g., ``UNKNOWN_LOC``) (``handle_nulls()``).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a436a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 3: NULL HANDLING =====\n",
    "def handle_nulls(chunk):\n",
    "    \"\"\"Impute nulls with placeholders (2014 rules)\"\"\"\n",
    "    return chunk.fillna({\n",
    "        'violation_location': 'UNKNOWN_LOC',  # 2014: 26.9% nulls\n",
    "        'issuer_command': 'UNKNOWN_CMD',      # 2014: 26% nulls  \n",
    "        'house_number': 'N/A',                # 2014: 33% nulls\n",
    "        'violation_county': 'UNKNOWN_COUNTY'  # 2014: 28.8% nulls\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3696a99",
   "metadata": {},
   "source": [
    "### **üìå STEP 4: String Cleaning**\n",
    "\n",
    "***Problem:*** Text fields had inconsistent casing and whitespace (e.g., ``\" toyota \"`` vs. ``\"TOYOTA\"``).\n",
    "\n",
    "***Solution:*** Trimmed and uppercased strings ``(clean_strings()``).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1ea7107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 4: STRING CLEANING =====\n",
    "def clean_strings(chunk):\n",
    "    \"\"\"Standardize text fields (2014 rules)\"\"\"\n",
    "    str_cols = ['plate_id', 'registration_state', 'vehicle_make', 'vehicle_body_type']\n",
    "    for col in str_cols:\n",
    "        if col in chunk.columns:\n",
    "            chunk[col] = chunk[col].astype(str).str.strip().str.upper()\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f0cdda",
   "metadata": {},
   "source": [
    "### **üìå STEP 5: State Filter (2014 only)**\n",
    "\n",
    "***Problem:*** 2014 data included invalid registration states.\n",
    "\n",
    "***Solution:*** Filtered to only valid states ``(filter_states()``).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa9f498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 5: STATE FILTER (2014 ONLY) ===== \n",
    "def filter_states(chunk):\n",
    "    \"\"\"Filter to valid US states (2014-specific)\"\"\"\n",
    "    if 'registration_state' in chunk.columns:\n",
    "        valid_states = ['NY','NJ','CT','PA','FL','CA']  # Your 2014 list\n",
    "        chunk = chunk[chunk['registration_state'].isin(valid_states)]\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dfad24",
   "metadata": {},
   "source": [
    "### **üìå STEP 6: Data Processing**\n",
    "\n",
    "***Problem:*** Dates were stored as strings (e.g., ``\"01/12/2014\"``).\n",
    "\n",
    "***Solution:*** Converted to datetime and extracted features (``issue_year``, ``issue_month``, etc.) ``(fix_dates()``).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9eb22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 6: DATE PROCESSING =====\n",
    "def fix_dates(chunk):\n",
    "    \"\"\"Convert and extract date features (2014 rules)\"\"\"\n",
    "    if 'issue_date' in chunk.columns:\n",
    "        chunk['issue_date'] = pd.to_datetime(chunk['issue_date'], errors='coerce')\n",
    "        chunk = chunk[chunk['issue_date'].notna()]\n",
    "        chunk['issue_year'] = chunk['issue_date'].dt.year\n",
    "        chunk['issue_month'] = chunk['issue_date'].dt.month\n",
    "        chunk['issue_day'] = chunk['issue_date'].dt.day\n",
    "        chunk['issue_dayofweek'] = chunk['issue_date'].dt.dayofweek  # Monday=0\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63522ec5",
   "metadata": {},
   "source": [
    "### **üìå STEP 7: Numeric Cleaning**\n",
    "\n",
    "***Problem:*** Numeric columns (e.g., ``violation_code``) were sometimes strings.\n",
    "\n",
    "***Solution:*** Coerced to numeric type ``(fix_numerics()``).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75a95d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 7: NUMERIC CLEANING =====\n",
    "def fix_numerics(chunk):\n",
    "    \"\"\"Ensure numeric columns are properly typed (2014 rules)\"\"\"\n",
    "    num_cols = ['violation_code', 'street_code1', 'street_code2', 'street_code3']\n",
    "    for col in num_cols:\n",
    "        if col in chunk.columns:\n",
    "            chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6de2a",
   "metadata": {},
   "source": [
    "### **üìå STEP 8: Duplicate Removal**\n",
    "\n",
    "***Problem:*** Duplicate ``summons_number`` entries skewed counts.\n",
    "\n",
    "***Solution:*** Tracked and removed duplicates across chunks ``(remove_duplicates()``).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a574d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 8: DUPLICATE REMOVAL =====\n",
    "def remove_duplicates(chunk, seen_summons):\n",
    "    \"\"\"Remove duplicate summons numbers (2014 rule)\"\"\"\n",
    "    if 'summons_number' in chunk.columns:\n",
    "        chunk = chunk[~chunk['summons_number'].isin(seen_summons)]\n",
    "        seen_summons.update(chunk['summons_number'].tolist())\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8968663",
   "metadata": {},
   "source": [
    "## üìÜ **3. Year-Specific Adjustments**\n",
    "\n",
    "***Problem:*** Schema changes year-over-year (e.g., column renames in 2016).\n",
    "\n",
    "***Solution:*** Applied year-specific fixes ``(apply_year_specific_rules()``):\n",
    "\n",
    "**2015:** Fixed time format (``\"A\"`` ‚Üí ``\"AM\"``).\n",
    "\n",
    "**2016:** Renamed ``vehicle_color_desc`` to ``vehicle_color``.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb6a2cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_year_specific_rules(chunk, year):\n",
    "    \"\"\"Handle schema changes per year\"\"\"\n",
    "    if year == 2015:\n",
    "        # Example: Fix time format change in 2015\n",
    "        if 'violation_time' in chunk.columns:\n",
    "            chunk['violation_time'] = chunk['violation_time'].str.replace('A','AM').replace('P','PM')\n",
    "    \n",
    "    elif year == 2016:\n",
    "        # Example: Column renamed in 2016\n",
    "        if 'vehicle_color_desc' in chunk.columns:\n",
    "            chunk = chunk.rename(columns={'vehicle_color_desc': 'vehicle_color'})\n",
    "    \n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d4f0a9",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è **4. Chunked Processing Pipeline**\n",
    "\n",
    "***Problem:*** Datasets were too large to load at once (8M‚Äì11M rows/year).\n",
    "\n",
    "***Solution:*** Processed in **100K-row chunks:**\n",
    "\n",
    "1. **Read:** Loaded CSV in chunks with error handling.\n",
    "\n",
    "2. **Clean:** Applied all cleaning steps sequentially.\n",
    "\n",
    "3. **Append:** Saved cleaned chunks incrementally.\n",
    "\n",
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "**2014:** 8.4M rows\n",
    "\n",
    "**2015:** 11.8M rows\n",
    "\n",
    "**2016:** 10.6M rows\n",
    "\n",
    "**2017:** 10.8M rows\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87bcfd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Starting 2014\n",
      "\n",
      "=== PROCESSING 2014 ===\n",
      "Input: raw_data/Parking_2014.csv\n",
      "Output: cleaned_data/cleaned_2014.csv\n",
      "Processing chunk 92...\n",
      "‚úÖ 2014 complete: 8,403,201 rows\n",
      "Completed 2014\n",
      "==============================\n",
      "\n",
      "\n",
      "==============================\n",
      "Starting 2015\n",
      "\n",
      "=== PROCESSING 2015 ===\n",
      "Input: raw_data/Parking_2015.csv\n",
      "Output: cleaned_data/cleaned_2015.csv\n",
      "Processing chunk 119...\n",
      "‚úÖ 2015 complete: 11,809,126 rows\n",
      "Completed 2015\n",
      "==============================\n",
      "\n",
      "\n",
      "==============================\n",
      "Starting 2016\n",
      "\n",
      "=== PROCESSING 2016 ===\n",
      "Input: raw_data/Parking_2016.csv\n",
      "Output: cleaned_data/cleaned_2016.csv\n",
      "Processing chunk 107...\n",
      "‚úÖ 2016 complete: 10,626,899 rows\n",
      "Completed 2016\n",
      "==============================\n",
      "\n",
      "\n",
      "==============================\n",
      "Starting 2017\n",
      "\n",
      "=== PROCESSING 2017 ===\n",
      "Input: raw_data/Parking_2017.csv\n",
      "Output: cleaned_data/cleaned_2017.csv\n",
      "Processing chunk 109...\n",
      "‚úÖ 2017 complete: 10,803,028 rows\n",
      "Completed 2017\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process_year(year):\n",
    "    input_path = f\"raw_data/Parking_{year}.csv\"\n",
    "    output_path = f\"cleaned_data/cleaned_{year}.csv\"\n",
    "    \n",
    "    print(f\"\\n=== PROCESSING {year} ===\")\n",
    "    print(f\"Input: {input_path}\")\n",
    "    print(f\"Output: {output_path}\")\n",
    "    \n",
    "    # Initialize with header only\n",
    "    pd.DataFrame(columns=get_expected_columns(year)).to_csv(output_path, index=False)\n",
    "    \n",
    "    # Track duplicates\n",
    "    seen_summons = set()\n",
    "    processed_rows = 0\n",
    "    chunk_counter = 0\n",
    "    \n",
    "    try:\n",
    "        # Read with more robust settings\n",
    "        reader = pd.read_csv(\n",
    "            input_path,\n",
    "            chunksize=CHUNK_SIZE,\n",
    "            dtype='unicode',  # Treat all as strings initially\n",
    "            encoding_errors='replace',\n",
    "            on_bad_lines='warn'\n",
    "        )\n",
    "        \n",
    "        for chunk in reader:\n",
    "            chunk_counter += 1\n",
    "            print(f\"Processing chunk {chunk_counter}...\", end='\\r')\n",
    "            \n",
    "            try:\n",
    "                # Apply cleaning pipeline\n",
    "                cleaned = (chunk\n",
    "                    .pipe(clean_column_names)\n",
    "                    .pipe(drop_useless_columns)\n",
    "                    .pipe(handle_nulls)\n",
    "                    .pipe(clean_strings))\n",
    "                \n",
    "                if year == 2014:\n",
    "                    cleaned = cleaned.pipe(filter_states)\n",
    "                \n",
    "                cleaned = (cleaned\n",
    "                    .pipe(fix_dates)\n",
    "                    .pipe(fix_numerics)\n",
    "                    .pipe(lambda x: remove_duplicates(x, seen_summons)))\n",
    "                \n",
    "                # Append to output\n",
    "                cleaned.to_csv(\n",
    "                    output_path,\n",
    "                    mode='a',\n",
    "                    header=False,\n",
    "                    index=False\n",
    "                )\n",
    "                processed_rows += len(cleaned)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError in chunk {chunk_counter}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\nFatal error processing {year}: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n‚úÖ {year} complete: {processed_rows:,} rows\")\n",
    "    return processed_rows\n",
    "\n",
    "# Helper function\n",
    "def get_expected_columns(year):\n",
    "    \"\"\"Returns expected columns after cleaning\"\"\"\n",
    "    base_cols = [\n",
    "        'summons_number', 'plate_id', 'registration_state', \n",
    "        'plate_type', 'issue_date', 'violation_code',\n",
    "        'vehicle_body_type', 'vehicle_make', 'issuing_agency',\n",
    "        'street_code1', 'street_code2', 'street_code3',\n",
    "        'vehicle_expiration_date', 'violation_location',\n",
    "        'violation_precinct', 'issuer_precinct', 'issuer_code',\n",
    "        'issuer_command', 'issuer_squad', 'violation_time',\n",
    "        'violation_county', 'house_number', 'street_name',\n",
    "        'date_first_observed', 'law_section', 'sub_division',\n",
    "        'days_parking_in_effect', 'vehicle_color',\n",
    "        'unregistered_vehicle?', 'vehicle_year', 'meter_number',\n",
    "        'violation_description', 'issue_year', 'issue_month',\n",
    "        'issue_day', 'issue_dayofweek'\n",
    "    ]\n",
    "    if year == 2016:\n",
    "        base_cols.remove('vehicle_color')\n",
    "        base_cols.append('vehicle_color_desc')\n",
    "    return base_cols\n",
    "\n",
    "# Process years with memory monitoring\n",
    "for year in YEARS:\n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(f\"Starting {year}\")\n",
    "    process_year(year)\n",
    "    print(f\"Completed {year}\")\n",
    "    print(f\"{'='*30}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75a0baa",
   "metadata": {},
   "source": [
    "## üîó **5. Combine All Years (Optional)**\n",
    "\n",
    "***Problem:*** Yearly files were split, complicating cross-year analysis.\n",
    "\n",
    "***Solution:*** Merged into a single dataset (combined_2014-2017.csv):\n",
    "\n",
    "**Total Rows:** 41.6M\n",
    "\n",
    "**Size:** ~5.2 GB (uncompressed)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b195a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2014...\n",
      "Processing 2015...\n",
      "Processing 2016...\n",
      "Processing 2017...\n",
      "‚úÖ Combined dataset created: 41,642,254 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "YEARS = [2014, 2015, 2016, 2017]\n",
    "OUTPUT_FILE = \"cleaned_data/combined_2014-2017.csv\"\n",
    "\n",
    "# Step 1: Initialize with header only\n",
    "first_file = f\"cleaned_data/cleaned_{YEARS[0]}.csv\"\n",
    "pd.read_csv(first_file, nrows=0).to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "# Step 2: Process each year in chunks\n",
    "for year in YEARS:\n",
    "    input_file = f\"cleaned_data/cleaned_{year}.csv\"\n",
    "    print(f\"Processing {year}...\")\n",
    "    \n",
    "    for chunk in pd.read_csv(input_file, chunksize=100000, low_memory=False):\n",
    "        chunk.to_csv(\n",
    "            OUTPUT_FILE,\n",
    "            mode='a',\n",
    "            header=False,\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "# Step 3: Verify\n",
    "row_count = sum(1 for _ in open(OUTPUT_FILE)) - 1  # Subtract header\n",
    "print(f\"‚úÖ Combined dataset created: {row_count:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2027379",
   "metadata": {},
   "source": [
    "## üîç **Future Analysis Opportunities**\n",
    "\n",
    "With clean data, we can now explore:\n",
    "\n",
    "1. **Trends:** Are violations increasing yearly?\n",
    "\n",
    "2. **Hotspots:** Which precincts issue the most tickets?\n",
    "\n",
    "3. **Vehicle Analysis:** Do certain car makes/colors get ticketed more?\n",
    "\n",
    "4. **Time Patterns:** Are tickets more common on weekdays or weekends?\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Conclusion**\n",
    "\n",
    "This pipeline transformed messy, fragmented data into a structured, analysis-ready dataset. By addressing inconsistencies, nulls, and schema changes, we‚Äôve unlocked the potential for deeper insights into NYC parking violations.\n",
    "\n",
    "**Key Takeaway:** Clean data is the foundation of impactful analysis. üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "**Tools Used:** Python, Pandas, Pathlib\n",
    "\n",
    "**Author:** Zahra Haider\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91688493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
